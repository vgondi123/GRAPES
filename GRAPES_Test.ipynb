{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4916bde-3f50-435e-86f7-ad14239e2fd2",
   "metadata": {},
   "source": [
    "# Benchmarking the GRAPES Deep Learning EEW Model on a Suite of California Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14be471-f0b7-4ba6-90a0-6427c275784d",
   "metadata": {},
   "source": [
    "Earthquake Early Warning (EEW) systems are designed to give people a heads-up before strong shaking starts. Traditional systems predict shaking by figuring out the size and location of an earthquake, but this can take a few seconds, potentially delaying the warning. The GRAPES EEW algorithm uses a new approach, powered by deep learning, to make faster and more accurate predictions. It looks at data from seismic sensors and connects them in a network, learning how shaking travels between stations. GRAPES analyzes patterns in the data every 4 seconds and updates predictions in real time.\n",
    "\n",
    "Originally trained on Japanese earthquake data, we're now testing GRAPES on California earthquakes to see how well it works here. In one test with the 2019 M7.1 Ridgecrest earthquake , the system warned all stations that needed alerts, with warning times ranging from immediate at the epicenter to over 30 seconds for stations further away. This shows the potential for GRAPES to help save lives by giving people more time to take cover. We’re continuing to evaluate it on 40 additional California earthquakes to measure its accuracy and warning times.\n",
    "\n",
    "The faster and more reliable these systems get, the more time people have to protect themselves before the shaking starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42308233-a016-4bb6-916e-7872f9783375",
   "metadata": {},
   "source": [
    "## 2019 M7.1 Ridgecrest Earthquake Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023cbe-ed9a-44bb-8879-186c3b417146",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Ridgecrest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c4a09-5420-491d-a119-04e635dfa4b3",
   "metadata": {},
   "source": [
    "### Modules & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    }
   ],
   "source": [
    "# I had to set SSL paths to install GRAPES.jl from Gitlab - this is an issue with Python / government firewalls\n",
    "import os\n",
    "os.environ[\"JULIA_SSL_CA_ROOTS_PATH\"] = \"\"\n",
    "\n",
    "# this imports julia as a python module called jl \n",
    "from juliacall import Main as jl\n",
    "\n",
    "# pkg is the Julia package installer \n",
    "from juliacall import Pkg as Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required julia packages into jl path \n",
    "jl.seval(\"using GRAPES\") \n",
    "jl.seval(\"using GraphNeuralNetworks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5eaf4f-4cc0-4120-bdf6-7e74733bb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "from obspy import read\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from obspy import Stream\n",
    "from obspy.geodetics import locations2degrees\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from scipy.signal import resample\n",
    "import glob\n",
    "from obspy.core.inventory import Inventory, Network, Station\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns\n",
    "from obspy import UTCDateTime\n",
    "from obspy import read_inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010f3fe-0249-41e4-89bf-403587b1ca8c",
   "metadata": {},
   "source": [
    "### Processing and Filtering Seismic Event Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ceffdb-7798-4282-b400-78b311aebb2e",
   "metadata": {},
   "source": [
    "The below code chunk downloads waveform data for specific seismic events from an AWS S3 bucket, organizes the data into directories based on the event date, and saves it locally in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c49d29a-f2a2-4d65-819f-d75edd60f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading event 38457511.ms 2024-09-19 13:34:25.693230\n",
      "download: s3://scedc-pds/event_waveforms/2019/2019_187/38457511.ms to ../data/scedc/event_waveforms/2019/2019_187/38457511.ms\n"
     ]
    }
   ],
   "source": [
    "#Downloading event wavefrom data\n",
    "\n",
    "# List of formatted dates and event IDs\n",
    "formatted_dates = [\n",
    "    \"2019/2019_187/\"\n",
    "]\n",
    "\n",
    "event_ids = [\n",
    "    \"38457511\",\n",
    "]\n",
    "\n",
    "# Working directories\n",
    "data_dir = \"../data/scedc/event_waveforms\"\n",
    "\n",
    "# Download each event's data\n",
    "for date_dir, event_id in zip(formatted_dates, event_ids):\n",
    "    day_dir = os.path.join(data_dir, date_dir)\n",
    "    os.makedirs(day_dir, exist_ok=True)\n",
    "    ms_filename = f\"{event_id}.ms\"\n",
    "    print(f\"Downloading event {ms_filename} {datetime.now()}\")\n",
    "\n",
    "    # Get file path\n",
    "    s3_path = os.path.join(\"s3://scedc-pds/event_waveforms/\", date_dir, ms_filename)\n",
    "    local_path = os.path.join(day_dir, ms_filename)\n",
    "\n",
    "    aws_str = f'aws s3 cp {s3_path} {local_path} --no-sign-request'\n",
    "    subprocess.call(aws_str, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816913d7-7851-44d2-ae79-804ee25c730d",
   "metadata": {},
   "source": [
    "The below code chunk focuses on preprocessing seismic waveform data for multiple events, ensuring data quality and consistency. It iterates through directories containing waveform files, filters by network and channel, and retains only stations with complete component data (East, North, Vertical). In this preliminary test, the code successfully processed around 900 traces, demonstrating its ability to clean and standardize data effectively. The process includes synchronizing traces by aligning start times, resampling data to a uniform 100 Hz sampling rate, and converting units from m/s² to cm/s². Stations with incomplete or duplicate data are excluded, and the final cleaned dataset for each event is stored in a dictionary for further analysis. In the overall test, the code scaled to process over 10,000 traces, highlighting its robustness and efficiency in managing large volumes of seismic data, ensuring that the data is thoroughly prepared for subsequent modeling or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487ef329-cae4-4ad7-9300-d35a275d7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing of the seismic data for each event\n",
    "\n",
    "# Base directory containing the waveform data\n",
    "base_directory = \"../data/scedc/event_waveforms/\"\n",
    "\n",
    "# Dictionary to store streams for each event\n",
    "event_streams = {}\n",
    "\n",
    "# Iterate through each year folder\n",
    "for year in os.listdir(base_directory):\n",
    "    year_directory = os.path.join(base_directory, year)\n",
    "   \n",
    "    if os.path.isdir(year_directory):\n",
    "        # Iterate through each event folder\n",
    "        for event in os.listdir(year_directory):\n",
    "            event_directory = os.path.join(year_directory, event)\n",
    "           \n",
    "            if os.path.isdir(event_directory):\n",
    "                # Initialize an empty Stream object for the event\n",
    "                st = obspy.Stream()\n",
    "               \n",
    "                # Iterate through all files in the event directory\n",
    "                for filename in os.listdir(event_directory):\n",
    "                    if filename.endswith(\".ms\"):\n",
    "                        file_path = os.path.join(event_directory, filename)\n",
    "                        try:\n",
    "                            st += read(file_path)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "\n",
    "                st = st.select(network=\"CI\")\n",
    "                st.merge()\n",
    "                st = st.select(channel=\"HN*\")\n",
    "                st = st.select(location=\"\")\n",
    "\n",
    "                stations_to_remove = {\"PDM\"}\n",
    "\n",
    "                st.traces = [trace for trace in st if trace.stats.station not in stations_to_remove]\n",
    "\n",
    "                station_components = {}\n",
    "                # Count the components for each station\n",
    "                for trace in st:\n",
    "                    station = trace.stats.station\n",
    "                    component = trace.stats.channel[-1]  # Last character indicates the component (E, N, Z)\n",
    "                    if station not in station_components:\n",
    "                        station_components[station] = set()\n",
    "                    station_components[station].add(component)\n",
    "               \n",
    "                # Keep only stations with all three components (E, N, Z)\n",
    "                stations_with_all_components = {station for station, components in station_components.items() if {'E', 'N', 'Z'} <= components}\n",
    "               \n",
    "                # Filter the stream manually for stations with all components and non-zero data\n",
    "                filtered_traces = [trace for trace in st\n",
    "                                   if trace.stats.station in stations_with_all_components and\n",
    "                                      not all(x == 0 for x in trace.data)]\n",
    "               \n",
    "                # Sort traces alphabetically by station code\n",
    "                sorted_traces = sorted(filtered_traces, key=lambda x: x.stats.station)\n",
    "               \n",
    "                # Create a new Stream from the filtered traces\n",
    "                st = st.__class__(sorted_traces)\n",
    "\n",
    "                st = st.split()\n",
    "\n",
    "                # Extract station names from each Trace\n",
    "                station_names = [trace.stats.station for trace in st]\n",
    "               \n",
    "                # Count the frequency of each station\n",
    "                station_counts = Counter(station_names)\n",
    "               \n",
    "                # Filter stations that appear more than 3 times\n",
    "                frequent_stations = {station: count for station, count in station_counts.items() if count > 3}\n",
    "               \n",
    "                # Filter out traces from the frequent stations\n",
    "                filtered_traces = [trace for trace in st if trace.stats.station not in frequent_stations]\n",
    "               \n",
    "                st = Stream(traces=filtered_traces)\n",
    "\n",
    "                # Create a new Stream for adjusted traces\n",
    "                adjusted_traces = []\n",
    "               \n",
    "                # Iterate through the Stream in batches of 3 traces\n",
    "                for i in range(0, len(st), 3):\n",
    "                    # Get the next 3 traces (assuming they are grouped by station)\n",
    "                    traces = st[i:i+3]\n",
    "                   \n",
    "                    if len(traces) == 3:\n",
    "                        # Find the maximum start time among these 3 traces\n",
    "                        max_start_time = max(trace.stats.starttime for trace in traces)\n",
    "                       \n",
    "                        # Trim all traces to start at the maximum start time\n",
    "                        for trace in traces:\n",
    "                            trace.trim(starttime=max_start_time)\n",
    "                            adjusted_traces.append(trace)\n",
    "               \n",
    "                # Create a new Stream with adjusted traces\n",
    "                st = Stream(traces=adjusted_traces)\n",
    "\n",
    "                #Checking sampling rate\n",
    "                for tr in st:\n",
    "                    if tr.stats.sampling_rate != 100:\n",
    "                        num_samples = int(tr.stats.npts * (100 / tr.stats.sampling_rate))\n",
    "                        tr.data = resample(tr.data, num_samples)\n",
    "                        tr.stats.sampling_rate = 100\n",
    "\n",
    "                #Converting from m/s^2 to cm/s^2\n",
    "                for tr in st:\n",
    "                    tr.data *= 100\n",
    "                   \n",
    "                # Store the raw stream in the event_streams dictionary\n",
    "                event_streams[event] = st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf37bb-e153-4f46-ab43-15556a3f5ebc",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Form.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879eb8d-2f3f-4e36-8f0d-52e844d79fb9",
   "metadata": {},
   "source": [
    "Example of a single waveform from the east channel of station ADO. This waveform captures the motion induced by the earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad82fd5-9a76-42e6-b726-65c6c927901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ObsPy UTCDateTime objects representing the specific dates and times of seismic events.\n",
    "dates = [\n",
    "    obspy.UTCDateTime(2019, 7, 6, 3, 19, 53)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2e421-671e-4437-883a-2a5c4ccedc48",
   "metadata": {},
   "source": [
    "This code chunk reads multiple StationXML files, which contain seismic station metadata, and combines them into a single Inventory object, specifically to gather station locations and remove sensitivities from the waveform data. By iterating through the files and appending the station data to the new network, the code ensures all relevant station locations are centralized. The resulting Inventory object provides a streamlined resource for working with station metadata, simplifying further seismic analysis by focusing on station locations while excluding sensitivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7131d38d-ca2c-4316-9e45-c63face3f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the StationXML files\n",
    "stationxml_directory_path = \"../data/scedc/FDSNstationXML/CI/*.xml\"\n",
    "\n",
    "# Read all StationXML files in the directory into Inventory objects\n",
    "stationxml_files = glob.glob(stationxml_directory_path)\n",
    "\n",
    "#Initialize an empty Network object\n",
    "inventory = Inventory(networks=[], source=\"Combined StationXML\")\n",
    "\n",
    "# Initialize a single Network object\n",
    "network_code = \"CI\"\n",
    "network = Network(code=network_code, stations=[])\n",
    "\n",
    "# Loop through the StationXML files and add them to the Network object\n",
    "for stationxml_file in stationxml_files:\n",
    "    # Read the StationXML file\n",
    "    inv = read_inventory(stationxml_file)\n",
    "   \n",
    "    # Loop through the stations in the Inventory and add them to the new network\n",
    "    for net in inv:\n",
    "        if net.code == network_code:\n",
    "            network.stations.extend(net.stations)\n",
    "\n",
    "# Add the network to the inventory\n",
    "inventory.networks.append(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f85a24-bac8-4321-b254-c5805d7e10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fruther preprocessing of the seismic data for each event:\n",
    "# 1. Remove any constant trend from the data.\n",
    "# 2. Apply a cosine taper to the first and last 5% of the data.\n",
    "# 3. Apply a high-pass filter with a cutoff frequency of 0.25 Hz to remove low-frequency noise.\n",
    "# 4. Correct the data using the station's instrument response based on the provided inventory.\n",
    "for event_key, st in event_streams.items():\n",
    "    st.detrend(\"constant\")\n",
    "    st.taper(max_percentage=0.05, type='cosine')\n",
    "    st.filter(\"highpass\", freq=0.25)\n",
    "    st.remove_sensitivity(inventory=inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ca003-0027-44b3-bd77-7f6eaf476804",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2988918-8c7e-4dab-9c6d-f9d08921e2c9",
   "metadata": {},
   "source": [
    "The following functions organize and prepare the data for processing by the GRAPES model, which relies on three critical inputs. First, the model requires a graph of station nodes, where nodes represent individual seismic stations linked by their geographic proximity and neighboring connections. Second, it needs acceleration waveforms that are segmented into 4-second windows and formatted into a 4-dimensional tensor to capture temporal dynamics effectively. Lastly, the model incorporates current maximum acceleration (PGA) values for each station, providing real-time insights into the intensity of seismic activity. These functions ensure that each input is accurately processed and structured to optimize the model's performance and predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_arrays(origin):\n",
    "    \"\"\"\n",
    "    Generate arrays of start and end times for 4-second windows.\n",
    "\n",
    "    This function creates two arrays: one for the start times and one for the end times of 4-second windows, \n",
    "    starting from the given origin time. The windows are generated sequentially with a 1-second step \n",
    "    from the origin up to 117 seconds later.\n",
    "\n",
    "    Args:\n",
    "        origin (obspy.UTCDateTime): The starting time for the windows.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two NumPy arrays:\n",
    "            - start_of_window (np.ndarray): Array of start times for each window.\n",
    "            - end_of_window (np.ndarray): Array of end times for each window.\n",
    "\n",
    "    Notes:\n",
    "        The length of both arrays will be 118, where each window is 4 seconds long, \n",
    "        and the start times are spaced 1 second apart.\n",
    "    \"\"\"\n",
    "    start_of_window = []\n",
    "    end_of_window = []\n",
    "\n",
    "    i = 1\n",
    "    start = origin\n",
    "    while start != origin + 117:\n",
    "        start_of_window.append(start)\n",
    "        end_of_window.append(start+4)\n",
    "        start += 1\n",
    "\n",
    "    start_of_window = np.array(start_of_window, dtype=obspy.UTCDateTime)\n",
    "    end_of_window = np.array(end_of_window, dtype=obspy.UTCDateTime)\n",
    "\n",
    "    return start_of_window, end_of_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(t1, t2, st):\n",
    "    \"\"\"\n",
    "    Extract a segment from a seismic Stream based on specified time window.\n",
    "\n",
    "    This function creates a new `Stream` object by copying the input stream and trimming it to the specified \n",
    "    start and end times. The resulting stream will only contain data within the given time window.\n",
    "\n",
    "    Args:\n",
    "        t1 (obspy.UTCDateTime): The start time of the desired window.\n",
    "        t2 (obspy.UTCDateTime): The end time of the desired window.\n",
    "        st (obspy.Stream): The input `Stream` object to be trimmed.\n",
    "\n",
    "    Returns:\n",
    "        obspy.Stream: A new `Stream` object containing data between `t1` and `t2`.\n",
    "\n",
    "    Notes:\n",
    "        The original `Stream` is not modified; a new `Stream` is returned.\n",
    "    \"\"\"\n",
    "    stream = st.copy().trim(starttime=t1, endtime=t2)\n",
    "    \n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_adjust_trace(tr, target):\n",
    "    \"\"\"\n",
    "    Adjust the number of samples in a trace to match a target size.\n",
    "\n",
    "    This function modifies the input trace to ensure it has exactly the specified number of samples. \n",
    "    If the trace has more samples than the target, it will be trimmed. If it has fewer samples, it \n",
    "    will be padded with zeros.\n",
    "\n",
    "    Args:\n",
    "        tr (obspy.Trace): The input trace to be adjusted.\n",
    "        target (int): The target number of samples for the trace.\n",
    "\n",
    "    Returns:\n",
    "        obspy.Trace: The modified trace with the number of samples adjusted to the target size.\n",
    "\n",
    "    Notes:\n",
    "        - The trace's `npts` attribute is updated to reflect the new number of samples.\n",
    "        - The trace's data is either trimmed or padded with zeros as necessary.\n",
    "    \"\"\"\n",
    "    if tr.stats.npts > target:\n",
    "        # Trim the trace to the target number of samples\n",
    "        tr.data = tr.data[:target]\n",
    "    elif tr.stats.npts < target:\n",
    "        # Pad the trace with zeros to the target number of samples\n",
    "        padding = target - tr.stats.npts\n",
    "        tr.data = np.pad(tr.data, (0, padding), 'constant', constant_values=0)\n",
    "    # Update the number of samples\n",
    "    tr.stats.npts = target\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_adjust_stream(target, st):\n",
    "    \"\"\"\n",
    "    Adjust the number of samples in each trace of a Stream to match a target size.\n",
    "\n",
    "    This function iterates through each trace in the input Stream and adjusts its number of samples \n",
    "    to the specified target size. Traces with the target number of samples are kept as they are, while \n",
    "    those with a different number of samples are adjusted using the `sample_adjust_trace` function. \n",
    "    A new `Stream` object is created with the adjusted traces.\n",
    "\n",
    "    Args:\n",
    "        target (int): The target number of samples for each trace.\n",
    "        st (obspy.Stream): The input Stream containing the traces to be adjusted.\n",
    "\n",
    "    Returns:\n",
    "        obspy.Stream: A new Stream object with traces adjusted to the target number of samples.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses `sample_adjust_trace` to handle the adjustment of individual traces.\n",
    "        - The original Stream is not modified; a new Stream with adjusted traces is returned.\n",
    "    \"\"\"\n",
    "    # Check and adjust each trace\n",
    "    adjusted_traces = []\n",
    "    for tr in st:\n",
    "        if tr.stats.npts == target:\n",
    "            adjusted_traces.append(tr)\n",
    "        else:\n",
    "            # Adjust the trace\n",
    "            adjusted_traces.append(sample_adjust_trace(tr, target))\n",
    "\n",
    "    #create a new Stream with the adjusted traces\n",
    "    st = Stream(traces=adjusted_traces)\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_unique_stations(st):\n",
    "    \"\"\"\n",
    "    Count the number of unique stations in a Stream.\n",
    "\n",
    "    This function extracts station names from each trace in the input Stream and calculates the number of \n",
    "    unique stations by counting distinct station names.\n",
    "\n",
    "    Args:\n",
    "        st (obspy.Stream): The input Stream containing traces from various stations.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of unique stations in the Stream.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that each trace in the Stream has a `station` attribute in its `stats`.\n",
    "    \"\"\"\n",
    "    # Extract station names\n",
    "    station_names = [tr.stats.station for tr in st]\n",
    "\n",
    "    # Count unique stations\n",
    "    unique_stations = set(station_names)\n",
    "    number_of_unique_stations = len(unique_stations)\n",
    "\n",
    "    return number_of_unique_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7640cdae-5082-42b7-bd66-84c1c44be6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_and_location(st, inventory, origin_lat, origin_long):\n",
    "    \"\"\"\n",
    "    Calculate distances from the epicenter and station locations.\n",
    "\n",
    "    This function computes the distance from a given epicenter to each seismic station, using only every \n",
    "    third trace (index 2) in the input Stream since each station contains 3 traces. It also collects the latitude and longitude of each station.\n",
    "\n",
    "    Args:\n",
    "        st (obspy.Stream): The input Stream containing traces from various stations.\n",
    "        inventory (obspy.Inventory): The inventory of seismic stations, including their coordinates.\n",
    "        origin_lat (float): The latitude of the epicenter.\n",
    "        origin_long (float): The longitude of the epicenter.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists:\n",
    "            - dist_from_epicenter_single (list): List of distances from the epicenter for each station.\n",
    "            - dist_from_epicenter_full (list): List of distances from the epicenter for each station's trace.\n",
    "            - station_location (list): List of tuples representing the latitude and longitude of each station.\n",
    "\n",
    "    Notes:\n",
    "        - The distance is calculated using the `locations2degrees` function.\n",
    "        - Only every third trace (based on zero-index count) is used for distance calculations.\n",
    "        - The `inventory` object is assumed to be organized by network and station.\n",
    "    \"\"\"\n",
    "    dist_from_epicenter_single = []\n",
    "    dist_from_epicenter_full = []\n",
    "    station_location = []\n",
    "    prev_station = \"\"\n",
    "    trace_count = 0\n",
    "    \n",
    "    for network in inventory:\n",
    "        for station in network:\n",
    "            for trace in st.select(station=station.code):\n",
    "                if trace_count % 3 == 2:  # Every third trace (index 2)\n",
    "                    x = locations2degrees(origin_lat, origin_long, station.latitude, station.longitude)\n",
    "                    y = (station.latitude, station.longitude)\n",
    "                    \n",
    "                    dist_from_epicenter_single.append(x)\n",
    "                    dist_from_epicenter_full.append(x)\n",
    "                    dist_from_epicenter_full.append(x)\n",
    "                    dist_from_epicenter_full.append(x)\n",
    "                    station_location.append(y)\n",
    "                trace_count += 1\n",
    "                \n",
    "    return dist_from_epicenter_single, dist_from_epicenter_full, station_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorter(dist_from_epicenter_single, dist_from_epicenter_full, station_location, st):\n",
    "    \"\"\"\n",
    "    Sort traces and associated data by distance from the epicenter.\n",
    "\n",
    "    This function sorts a Stream of seismic traces and associated distance arrays based on distances from the epicenter.\n",
    "    The traces are ordered according to their distances, and the corresponding distance and location arrays are \n",
    "    re-ordered to match this sorting.\n",
    "\n",
    "    Args:\n",
    "        dist_from_epicenter_single (list): List of distances from the epicenter for each station.\n",
    "        dist_from_epicenter_full (list): List of distances from the epicenter for each station's trace.\n",
    "        station_location (list of tuples): List of tuples representing the latitude and longitude of each station.\n",
    "        st (obspy.Stream): The input Stream containing seismic traces that need to be sorted.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - ordered_stream (obspy.Stream): The Stream sorted by distance from the epicenter.\n",
    "            - dist_from_epicenter_single_array (np.ndarray): Array of distances distances from the epicenter for each station, sorted.\n",
    "            - dist_from_epicenter_full_array (np.ndarray): Array of distances from the epicenter for each station's trace, sorted.\n",
    "            - station_location_array (np.ndarray): Array of tuples representing the latitude and longitude of each station, sorted.\n",
    "\n",
    "    Notes:\n",
    "        - The sorting is performed based on the distances provided in `dist_from_epicenter_full` and 'dist_from_epicenter_single'.\n",
    "        - The distances and locations are converted to NumPy arrays for efficient indexing and sorting.\n",
    "    \"\"\"\n",
    "    dist_idx = np.argsort(dist_from_epicenter_full)\n",
    "    dist_idx_2 = np.argsort(dist_from_epicenter_single)\n",
    "\n",
    "    #Sorting stream by distance\n",
    "    ordered_stream = obspy.Stream([st[ii] for ii in dist_idx])\n",
    "\n",
    "    #Sorting distance array\n",
    "    dist_from_epicenter_single_array = np.array(dist_from_epicenter_single, dtype=float)\n",
    "    dist_from_epicenter_single_array = dist_from_epicenter_single_array[dist_idx_2] \n",
    "\n",
    "    #Sorting distance total array\n",
    "    dist_from_epicenter_full_array = np.array(dist_from_epicenter_full, dtype=float)\n",
    "    dist_from_epicenter_full_array = dist_from_epicenter_full_array[dist_idx] \n",
    "\n",
    "    #Sorting station location array\n",
    "    station_location_array = np.array(station_location, dtype=float)\n",
    "    station_location_array = station_location_array[dist_idx_2]\n",
    "\n",
    "    return ordered_stream, dist_from_epicenter_single_array, dist_from_epicenter_full_array, station_location_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(station_location_array):\n",
    "    \"\"\"\n",
    "    Compute a matrix of distances between pairs of stations.\n",
    "\n",
    "    This function calculates the distance between each pair of stations using their latitude and longitude \n",
    "    coordinates and returns a distance matrix. The distance is computed using the `locations2degrees` function.\n",
    "\n",
    "    Args:\n",
    "        station_location_array (list of tuples): A list where each tuple contains the latitude and longitude \n",
    "            of a station in the form (latitude, longitude).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array where each element [i, j] represents the distance between the i-th and j-th\n",
    "            stations, calculated using the `locations2degrees` function.\n",
    "\n",
    "    Notes:\n",
    "        - The distance matrix will be square, with dimensions equal to the number of stations.\n",
    "        - The function assumes that `station_location_array` contains valid latitude and longitude pairs.\n",
    "    \"\"\"\n",
    "    dist_matrix = np.zeros((len(station_location_array), len(station_location_array)))\n",
    "    for i in range(len(station_location_array)):\n",
    "        for j in range(len(station_location_array)):\n",
    "            dist_matrix[i][j] = locations2degrees(station_location_array[i][0], station_location_array[i][1], station_location_array[j][0], station_location_array[j][1])\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix(dist_matrix, station_location_array):\n",
    "    \"\"\"\n",
    "    Generate an adjacency matrix based on the k-nearest neighbors of each station.\n",
    "\n",
    "    This function creates an adjacency matrix where each entry indicates whether two stations are \n",
    "    connected based on their k-nearest neighbors.\n",
    "\n",
    "    Args:\n",
    "        dist_matrix (np.ndarray): A 2D NumPy array representing the distance matrix, where each element [i, j]\n",
    "            denotes the distance between the i-th and j-th stations.\n",
    "        station_location_array (list of tuples): A list of tuples containing the latitude and longitude\n",
    "            of each station.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array (adjacency matrix) where each element [i, j] is 1 if the i-th and j-th \n",
    "            stations are among each other's 10 nearest neighbors, and 0 otherwise.\n",
    "\n",
    "    Notes:\n",
    "        - The adjacency matrix is symmetric, meaning if station i is connected to station j, then station j \n",
    "            is also connected to station i.\n",
    "        - The function assumes that `dist_matrix` is a square matrix with the number of stations as its dimensions.\n",
    "    \"\"\"\n",
    "    #Creating k = 10 adjacency matrix\n",
    "    adj_matrix = np.zeros_like(dist_matrix)\n",
    "\n",
    "    for i in range(len(station_location_array)):\n",
    "        # Get the indices of the k nearest neighbors, excluding the point itself\n",
    "        nearest_neighbors = np.argsort(dist_matrix[i])[1:11]\n",
    "        \n",
    "        # Set the corresponding entries in the adjacency matrix to 1\n",
    "        adj_matrix[i, nearest_neighbors] = 1\n",
    "        adj_matrix[nearest_neighbors, i] = 1\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611e9fc-111e-46ea-99fa-0b3af4588f20",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93347edd-542c-4f75-bcad-aa566d872e0f",
   "metadata": {},
   "source": [
    "The GRAPES model utilizes a Graph Neural Network (GNN) to predict earthquake shaking by leveraging both station-level and network-level information. At each seismic station (represented as nodes in the graph), real-time waveform data is processed to extract key features, such as phase and amplitude, forming feature vectors that represent the waveforms. These feature vectors are then shared with neighboring stations using the GNN, where each station communicates with its nearby stations (within K hops, such as K=1 or K=2). For the test, K=10 neighbors were used, and this was implemented by creating an adjacency matrix to define the relationships between stations. The GNN aggregates and updates the features from its neighbors, refining the information available at each node. Finally, the model predicts Peak Ground Acceleration (PGA) at each station using the refined features, allowing the system to estimate shaking intensity in real time as the earthquake rupture evolves. This combination of local waveform data and network-wide feature sharing helps improve the prediction of ground shaking across the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_maker(num_unique_stations, st):\n",
    "    \"\"\"\n",
    "    Create a 4D tensor from seismic trace data for multiple stations.\n",
    "\n",
    "    This function generates a tensor of shape `(400, 3, 1, num_unique_stations)`, where:\n",
    "    - The first dimension (400) represents the number of samples in each trace (assuming each trace has 400 data points).\n",
    "    - The second dimension (3) corresponds to the three seismic components: East, North, and Vertical.\n",
    "    - The third dimension (1) is for broadcasting purposes.\n",
    "    - The fourth dimension corresponds to the number of unique seismic stations.\n",
    "\n",
    "    Args:\n",
    "        num_unique_stations (int): The total number of unique stations in the Stream.\n",
    "        st (obspy.Stream): The input Stream, where traces are ordered by station and in groups of three (East, North, Vertical).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 4D tensor of shape `(400, 3, 1, num_unique_stations)` containing the seismic data for each station.\n",
    "\n",
    "    Notes:\n",
    "        - Each station is expected to have exactly three traces in the Stream: East, North, and Vertical components, in that order.\n",
    "        - The tensor is filled such that for each station, the data from its traces are assigned to the corresponding component in the tensor.\n",
    "        - The resulting tensor is cast to `np.float32` for efficiency in deep learning applications.\n",
    "    \"\"\"\n",
    "    shape = (400, 3, 1, num_unique_stations)\n",
    "\n",
    "    # Create the empty array\n",
    "    X = np.empty(shape)\n",
    "\n",
    "    x = 0\n",
    "    for i in range(0, len(st), 3):\n",
    "        # Extract the components for the current station\n",
    "        east_trace = st[i]\n",
    "        north_trace = st[i+1]\n",
    "        vertical_trace = st[i+2]\n",
    "    \n",
    "        # Fill the array X with the respective component data\n",
    "        X[:, 0, 0, x] = east_trace.data\n",
    "        X[:, 1, 0, x] = north_trace.data\n",
    "        X[:, 2, 0, x] = vertical_trace.data\n",
    "\n",
    "        x+=1\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e9023-6cc3-4c44-a8f6-fc49e21a9ee8",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b1135-9169-454a-9381-028cb383bbd2",
   "metadata": {},
   "source": [
    "The image depicts the structure of an acceleration waveform tensor used as input for the GRAPES model, which contains seismic data from multiple stations. Each station provides 400 samples representing 4 seconds of data across three waveform channels: East (E), North (N), and Vertical (Z). This data is organized into tensors, allowing the model to process seismic information from multiple stations simultaneously. By capturing waveform features over 4-second windows, the GRAPES model can analyze seismic activity in real-time, utilizing this structured input to predict ground shaking intensity across the network of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pga_calculator(st):\n",
    "    \"\"\"\n",
    "    Calculate Peak Ground Acceleration (PGA) for each station in the seismic data.\n",
    "\n",
    "    This function computes the logarithm of the maximum amplitude (PGA) for each station using the three seismic components (East, North, and Vertical). \n",
    "    The PGA is calculated by combining the squared amplitude of each component and taking the square root of the sum, followed by calculating the log base 10 of the maximum value.\n",
    "\n",
    "    Args:\n",
    "        st (obspy.Stream): The input Stream, where traces are ordered by station and in groups of three (East, North, Vertical).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PGA values (log10 of the maximum amplitude) for each station.\n",
    "\n",
    "    Notes:\n",
    "        - The input Stream is expected to have traces grouped by station, with three traces for each station: East, North, and Vertical components.\n",
    "        - The function computes the combined amplitude for each station by summing the squares of the individual components and then taking the square root of the sum.\n",
    "        - The logarithm (base 10) of the maximum amplitude is returned for each station.\n",
    "    \"\"\"\n",
    "    pga = []\n",
    "    for i in range(0, len(st), 3):\n",
    "        # Extract the components for the current station\n",
    "        east_trace = st[i]\n",
    "        north_trace = st[i+1]\n",
    "        vertical_trace = st[i+2]\n",
    "    \n",
    "        # Fill the array X with the respective component data\n",
    "        amp = east_trace.data ** 2\n",
    "        amp += north_trace.data ** 2\n",
    "        amp += vertical_trace.data ** 2\n",
    "        np.sqrt(amp, out=amp)\n",
    "        pga.append(np.log10(np.max(amp)))\n",
    "\n",
    "    return pga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b10dd-971b-4af1-b315-57b2b24b6a67",
   "metadata": {},
   "source": [
    "### Running The GRAPES Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cf675-d371-42c8-850a-9f9d4bc3a496",
   "metadata": {},
   "source": [
    "This code chunk processes seismic event data using the GRAPES model and integrates the functions mentioned before for this purpose. It iterates through each event, defining time windows and filtering the seismic network inventory to include only relevant stations. Stream data is segmented into 4-second windows and adjusted to ensure completeness, using functions such as window, sample_adjust_stream, and sorter. Distance calculations generate distance and adjacency matrices, which are then used to prepare tensors and Peak Ground Acceleration (PGA) values. These inputs are fed into the GRAPES model, leveraging the functions tensor_maker, pga_calculator, and adjacency_matrix. The model produces predictions, which, along with actual values, are organized into a dictionary and collected for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58adba-0b2f-481b-8cee-b706574a0280",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Grapes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692df508-fadf-49a6-b075-a05999752490",
   "metadata": {},
   "source": [
    "In the GRAPES Earthquake Early Warning system, both Convolutional Neural Networks (CNNs) and Dense Neural Networks (DNNs) play a crucial role in processing seismic data. The CNN first processes seismic waveform data by analyzing short 4-second windows of ground motion using 1D convolutions. These convolutions identify key patterns, such as the amplitude and phase of seismic waves, that are critical for understanding how the earthquake is progressing. The CNN transforms this raw waveform data into a set of high-level feature vectors, capturing essential information from the waveforms. These feature vectors are then passed to a Dense Neural Network (DNN), which further refines and combines these features. The DNN processes the extracted features to make more accurate predictions about the intensity and spread of the earthquake’s shaking. By stacking layers of fully connected neurons, the DNN helps the model understand complex relationships in the data, improving the accuracy of the final predictions about ground shaking and warning times at different seismic stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running GRAPES Model\n",
    "collection = []\n",
    "i=0\n",
    "model = jl.load_GRAPES_model()\n",
    "for event_key, st in event_streams.items():\n",
    "    start_of_window, end_of_window = window_arrays(dates[i])\n",
    "   \n",
    "    inv = inventory.copy()\n",
    "   \n",
    "    # Create a set of station identifiers (network.code) that are present in the stream\n",
    "    stream_stations = set((trace.stats.network, trace.stats.station) for trace in st)\n",
    "   \n",
    "    # Filter the inventory to keep only stations that are in the stream\n",
    "    for network in inv:\n",
    "        stations_to_remove = []\n",
    "        for station in network:\n",
    "            # Use both network and station code to identify the station\n",
    "            station_id = (network.code, station.code)\n",
    "            if station_id not in stream_stations:\n",
    "                stations_to_remove.append(station)\n",
    "        for station in stations_to_remove:\n",
    "            network.stations.remove(station)\n",
    "\n",
    "\n",
    "    stations_to_remove = []\n",
    "\n",
    "    prev_station_code = None\n",
    "   \n",
    "    for network in inv:\n",
    "        for station in network.stations:\n",
    "            if prev_station_code is not None and prev_station_code == station.code:\n",
    "                #Store the previous station in the list for removal\n",
    "                stations_to_remove.append((network, prev_station))\n",
    "            #Set prev_station to the current station\n",
    "            prev_station = station\n",
    "            prev_station_code = station.code\n",
    "   \n",
    "    #Now remove the stations after the loop\n",
    "    for network, station in stations_to_remove:\n",
    "        network.stations.remove(station)\n",
    "\n",
    "    data_dict = {}\n",
    "   \n",
    "    for j in range(len(start_of_window)):\n",
    "        temp_st = window(start_of_window[j], end_of_window[j], st)\n",
    "        temp_st = sample_adjust_stream(400, temp_st)\n",
    "\n",
    "        if len(temp_st) != 0:\n",
    "            d = temp_st._groupby(\"{network}.{station}\")\n",
    "            for key, value in d.items():\n",
    "                if len(value) != 3:\n",
    "                    for tr in value:\n",
    "                        temp_st.remove(tr)\n",
    "       \n",
    "            num_stations = num_unique_stations(temp_st)\n",
    "       \n",
    "            dist_from_epicenter_single, dist_from_epicenter_full, station_location = distance_and_location(temp_st, inv, 38.21550, -122.31167)\n",
    "           \n",
    "            temp_st, dist_from_epicenter_single_array, dist_from_epicenter_full_array, station_location_array = sorter(dist_from_epicenter_single, dist_from_epicenter_full, station_location, temp_st)\n",
    "       \n",
    "            dist_matrix = distance_matrix(station_location_array)\n",
    "       \n",
    "            adj_matrix = adjacency_matrix(dist_matrix, station_location_array)\n",
    "           \n",
    "            X = tensor_maker(num_stations, temp_st)\n",
    "       \n",
    "            pga = pga_calculator(temp_st)\n",
    "       \n",
    "            pred = model(jl.GNNGraph(adj_matrix, ndata = X, gdata= pga))\n",
    "            pred_values = np.array(pred.ndata.x).flatten()\n",
    "            actual_values = np.array(pred.gdata.u).flatten()\n",
    "       \n",
    "            # Initialize a list to hold all data temporarily\n",
    "            all_station_data = []\n",
    "           \n",
    "            # Main loop\n",
    "            for k in range(len(station_location_array)):\n",
    "                station_code = temp_st[3 * k].stats.station\n",
    "                station_data = {\n",
    "                    \"time_window\": j,\n",
    "                    \"location\": station_location_array[k],\n",
    "                    \"prediction\": pred_values[k],\n",
    "                    \"actual\": actual_values[k]\n",
    "                }\n",
    "                # Collect the station code and data in a list\n",
    "                all_station_data.append((station_code, station_data))\n",
    "           \n",
    "            # Update data_dict after the loop\n",
    "            for station_code, station_data in all_station_data:\n",
    "                if station_code not in data_dict:\n",
    "                    data_dict[station_code] = []\n",
    "                data_dict[station_code].append(station_data)\n",
    "\n",
    "    collection.append(data_dict)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e6da3-99e9-4582-8fdd-dd0ec9efc6c2",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d5b91-f189-4948-8b07-3580e0d923e2",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/PGA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bd325-e86f-4801-84d5-b2ae34dd6fef",
   "metadata": {},
   "source": [
    "These two maps compare the predicted and actual Peak Ground Acceleration (PGA) values at seismic stations across the region for the Ridgecrest EQ. In the left map, the predicted PGA values (maximum during the event) are shown, while the right map displays the actual recorded PGA. Both maps use a color gradient, with lighter colors indicating higher PGA values (on a logarithmic scale). The red star marks the earthquake’s epicenter. We can see that there is overprediction of PGA from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3cbd6-32b9-4cf6-81eb-ddb520bc8928",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cddd24-364c-43cb-ba9b-4d7ea34a8d5c",
   "metadata": {},
   "source": [
    "A scatter plot comparing the observed Modified Mercalli Intensity (MMI) to the predicted MMI values at each station for the Ridgecrest EQ. The colored dots indicate the status of each station’s prediction (True Positive, True Negative, False Positive, False Negative), with red dashed lines marking the MMI threshold of 4. A warning alert is sent out when the model first detects an MMI of at least 4. We can see that the model either unnecessarily sent out an alert or correctly sent out an alert. The model did not have an instance where it did not send out an alert when it should have. This is important as failing to send an alert is the worst possible outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf76fc9-15ac-4523-80b8-e585e2f33a10",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c658c-bd12-402a-b748-c06d3e8b61ab",
   "metadata": {},
   "source": [
    "A scatter plot showing the relationship between warning time (in seconds) and the distance from the earthquake's origin (in kilometers) for the Ridgecrest EQ. The plot demonstrates how warning times increase with distance from the epicenter. This is the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910abf4b-1de5-4b21-8ef5-7165d8e08224",
   "metadata": {},
   "source": [
    "![My Image](../../../Desktop/Warning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61d5c7-39cb-40ce-95b1-40edfcd9cd6a",
   "metadata": {},
   "source": [
    "This graph compares the predicted and observed MMI values over time for seismic station CRR during the Ridgecrest EQ. The blue line represents the observed shaking, while the red line shows the predicted peak shaking intensity. The green dashed line marks the MMI = 4 threshold, which indicates when the shaking becomes strong enough to issue warnings. The system predicted the shaking would reach MMI 4 about 40 seconds before it was actually observed at the station, providing a significant warning time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819236a-5450-4a32-b6b8-4b2d457ff2d1",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908213f-9050-4353-8462-6e2d0c29704e",
   "metadata": {},
   "source": [
    "The GRAPES algorithm demonstrated an overprediction of Peak Ground Acceleration (PGA) during the Ridgecrest Earthquake and other events in the test suite. This overprediction could be due to the need for retraining on California-specific data, as the region’s geophysical features differs significantly from Japan, where GRAPES was initially tested. However, overprediction in earthquake early warning systems is generally less concerning, as it is better to issue an unnecessary alert than to fail to send one in a critical situation.\n",
    "\n",
    "Importantly, GRAPES provided good warning times in California. During the Ridgecrest Earthquake, the USGS' ShakeAlert Early Warning System failed to send an alert to Los Angeles County. In contrast, GRAPES successfully issued an alert, providing 30-40 seconds of warning time. This advance notice is crucial in earthquake early warning systems, as every second counts for saving lives, reducing injuries, and allowing people to take protective actions. The ability to deliver timely warnings is one of the most critical aspects of an effective early warning system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
